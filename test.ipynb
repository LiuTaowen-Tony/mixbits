{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-1, -1, -1, -1),\n",
       " (-1, -1, -1, 1),\n",
       " (-1, -1, 1, -1),\n",
       " (-1, -1, 1, 1),\n",
       " (-1, 1, -1, -1),\n",
       " (-1, 1, -1, 1),\n",
       " (-1, 1, 1, -1),\n",
       " (-1, 1, 1, 1),\n",
       " (1, -1, -1, -1),\n",
       " (1, -1, -1, 1),\n",
       " (1, -1, 1, -1),\n",
       " (1, -1, 1, 1),\n",
       " (1, 1, -1, -1),\n",
       " (1, 1, -1, 1),\n",
       " (1, 1, 1, -1),\n",
       " (1, 1, 1, 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import itertools \n",
    "inputs = list(itertools.product([-1, 1], repeat=4))\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_possible_labels  = []\n",
    "for i in itertools.product([-1, 1], repeat=16):\n",
    "    all_possible_labels.append(torch.tensor(i, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,\n",
       "        -1., -1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_possible_labels[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lut2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lut2, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2 = x\n",
    "        x = torch.tensor([x1, x2, x1*x2], dtype=torch.float32)\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class Lut4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lut4, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(15, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2, x3, x4 = x\n",
    "        x = torch.tensor([x1, x2, x3, x4, \n",
    "                          x1*x2, x1*x3, x1*x4, x2*x3, x2*x4, x3*x4,\n",
    "                          x1*x2*x3, x1*x2*x4, x1*x3*x4, x2*x3*x4,\n",
    "                          x1*x2*x3*x4], dtype=torch.float32)\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.lut1 = Lut2()\n",
    "        self.lut2 = Lut2()\n",
    "        self.lut3 = Lut2()\n",
    "        self.debug = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2, x3, x4 = x\n",
    "        y1 = self.lut1([x1, x2])\n",
    "        y2 = self.lut2([x3, x4])\n",
    "        y1 = torch.where(y1 > 0, 1, -1)\n",
    "        y2 = torch.where(y2 > 0, 1, -1)\n",
    "        if self.debug:\n",
    "            print(y1, y2)\n",
    "        return self.lut3([y1, y2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tl2020/mixbits/.venv/lib64/python3.9/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.33074048161506653\n",
      "Epoch 100 Loss: 5.403677505455562e-10\n",
      "Epoch 200 Loss: 2.7853275241795927e-12\n",
      "Epoch 300 Loss: 2.7853275241795927e-12\n",
      "Epoch 400 Loss: 2.7853275241795927e-12\n",
      "Epoch 500 Loss: 2.7853275241795927e-12\n",
      "Epoch 600 Loss: 2.7853275241795927e-12\n",
      "Epoch 700 Loss: 2.7853275241795927e-12\n",
      "Epoch 800 Loss: 2.7853275241795927e-12\n",
      "Epoch 900 Loss: 2.7853275241795927e-12\n",
      "Final Loss: 2.7853275241795927e-12\n",
      "Epoch 0 Loss: 0.5481227040290833\n",
      "Epoch 100 Loss: 9.459313332627062e-10\n",
      "Epoch 200 Loss: 1.566746732351021e-12\n",
      "Epoch 300 Loss: 1.566746732351021e-12\n",
      "Epoch 400 Loss: 1.566746732351021e-12\n",
      "Epoch 500 Loss: 1.566746732351021e-12\n",
      "Epoch 600 Loss: 1.566746732351021e-12\n",
      "Epoch 700 Loss: 1.566746732351021e-12\n",
      "Epoch 800 Loss: 1.566746732351021e-12\n",
      "Epoch 900 Loss: 1.566746732351021e-12\n",
      "Final Loss: 1.566746732351021e-12\n",
      "Epoch 0 Loss: 0.24808713793754578\n",
      "Epoch 100 Loss: 4.228617456192296e-10\n",
      "Epoch 200 Loss: 2.2737367544323206e-13\n",
      "Epoch 300 Loss: 2.2737367544323206e-13\n",
      "Epoch 400 Loss: 2.2737367544323206e-13\n",
      "Epoch 500 Loss: 2.2737367544323206e-13\n",
      "Epoch 600 Loss: 2.2737367544323206e-13\n",
      "Epoch 700 Loss: 2.2737367544323206e-13\n",
      "Epoch 800 Loss: 2.2737367544323206e-13\n",
      "Epoch 900 Loss: 2.2737367544323206e-13\n",
      "Final Loss: 2.2737367544323206e-13\n",
      "Epoch 0 Loss: 0.996169924736023\n",
      "Epoch 100 Loss: 1.6865477903138526e-09\n",
      "Epoch 200 Loss: 1.0267342531733448e-12\n",
      "Epoch 300 Loss: 1.0267342531733448e-12\n",
      "Epoch 400 Loss: 1.0267342531733448e-12\n",
      "Epoch 500 Loss: 1.0267342531733448e-12\n",
      "Epoch 600 Loss: 1.0267342531733448e-12\n",
      "Epoch 700 Loss: 1.0267342531733448e-12\n",
      "Epoch 800 Loss: 1.0267342531733448e-12\n",
      "Epoch 900 Loss: 1.0267342531733448e-12\n",
      "Final Loss: 1.0267342531733448e-12\n",
      "Epoch 0 Loss: 0.11944402009248734\n",
      "Epoch 100 Loss: 2.0463630789890885e-10\n",
      "Epoch 200 Loss: 3.552713678800501e-13\n",
      "Epoch 300 Loss: 3.552713678800501e-13\n",
      "Epoch 400 Loss: 3.552713678800501e-13\n",
      "Epoch 500 Loss: 3.552713678800501e-13\n",
      "Epoch 600 Loss: 3.552713678800501e-13\n",
      "Epoch 700 Loss: 3.552713678800501e-13\n",
      "Epoch 800 Loss: 3.552713678800501e-13\n",
      "Epoch 900 Loss: 3.552713678800501e-13\n",
      "Final Loss: 3.552713678800501e-13\n",
      "Epoch 0 Loss: 2.188646078109741\n",
      "Epoch 100 Loss: 3.739856424544996e-09\n",
      "Epoch 200 Loss: 3.8689051962137455e-12\n",
      "Epoch 300 Loss: 3.8689051962137455e-12\n",
      "Epoch 400 Loss: 3.8689051962137455e-12\n",
      "Epoch 500 Loss: 3.8689051962137455e-12\n",
      "Epoch 600 Loss: 3.8689051962137455e-12\n",
      "Epoch 700 Loss: 3.8689051962137455e-12\n",
      "Epoch 800 Loss: 3.8689051962137455e-12\n",
      "Epoch 900 Loss: 3.8689051962137455e-12\n",
      "Final Loss: 3.8689051962137455e-12\n",
      "Epoch 0 Loss: 0.19497689604759216\n",
      "Epoch 100 Loss: 3.283275873400271e-10\n",
      "Epoch 200 Loss: 5.115907697472721e-13\n",
      "Epoch 300 Loss: 5.115907697472721e-13\n",
      "Epoch 400 Loss: 5.115907697472721e-13\n",
      "Epoch 500 Loss: 5.115907697472721e-13\n",
      "Epoch 600 Loss: 5.115907697472721e-13\n",
      "Epoch 700 Loss: 5.115907697472721e-13\n",
      "Epoch 800 Loss: 5.115907697472721e-13\n",
      "Epoch 900 Loss: 5.115907697472721e-13\n",
      "Final Loss: 5.115907697472721e-13\n",
      "Epoch 0 Loss: 0.18463590741157532\n",
      "Epoch 100 Loss: 3.197442310920451e-10\n",
      "Epoch 200 Loss: 1.7195134205394424e-12\n",
      "Epoch 300 Loss: 1.7195134205394424e-12\n",
      "Epoch 400 Loss: 1.7195134205394424e-12\n",
      "Epoch 500 Loss: 1.7195134205394424e-12\n",
      "Epoch 600 Loss: 1.7195134205394424e-12\n",
      "Epoch 700 Loss: 1.7195134205394424e-12\n",
      "Epoch 800 Loss: 1.7195134205394424e-12\n",
      "Epoch 900 Loss: 1.7195134205394424e-12\n",
      "Final Loss: 1.7195134205394424e-12\n",
      "Epoch 0 Loss: 3.7628066539764404\n",
      "Epoch 100 Loss: 6.446075673238738e-09\n",
      "Epoch 200 Loss: 1.7195134205394424e-12\n",
      "Epoch 300 Loss: 1.7195134205394424e-12\n",
      "Epoch 400 Loss: 1.7195134205394424e-12\n",
      "Epoch 500 Loss: 1.7195134205394424e-12\n",
      "Epoch 600 Loss: 1.7195134205394424e-12\n",
      "Epoch 700 Loss: 1.7195134205394424e-12\n",
      "Epoch 800 Loss: 1.7195134205394424e-12\n",
      "Epoch 900 Loss: 1.7195134205394424e-12\n",
      "Final Loss: 1.7195134205394424e-12\n",
      "Epoch 0 Loss: 2.7454733848571777\n",
      "Epoch 100 Loss: 4.6902961514661e-09\n",
      "Epoch 200 Loss: 6.963318810448982e-13\n",
      "Epoch 300 Loss: 6.963318810448982e-13\n",
      "Epoch 400 Loss: 6.963318810448982e-13\n",
      "Epoch 500 Loss: 6.963318810448982e-13\n",
      "Epoch 600 Loss: 6.963318810448982e-13\n",
      "Epoch 700 Loss: 6.963318810448982e-13\n",
      "Epoch 800 Loss: 6.963318810448982e-13\n",
      "Epoch 900 Loss: 6.963318810448982e-13\n",
      "Final Loss: 6.963318810448982e-13\n",
      "Epoch 0 Loss: 0.8289153575897217\n",
      "Epoch 100 Loss: 1.4190391084412113e-09\n",
      "Epoch 200 Loss: 4.604316927725449e-12\n",
      "Epoch 300 Loss: 4.604316927725449e-12\n",
      "Epoch 400 Loss: 4.604316927725449e-12\n",
      "Epoch 500 Loss: 4.604316927725449e-12\n",
      "Epoch 600 Loss: 4.604316927725449e-12\n",
      "Epoch 700 Loss: 4.604316927725449e-12\n",
      "Epoch 800 Loss: 4.604316927725449e-12\n",
      "Epoch 900 Loss: 4.604316927725449e-12\n",
      "Final Loss: 4.106937012693379e-12\n",
      "Epoch 0 Loss: 1.528721809387207\n",
      "Epoch 100 Loss: 2.591050929368066e-09\n",
      "Epoch 200 Loss: 1.4210854715202004e-12\n",
      "Epoch 300 Loss: 1.4210854715202004e-12\n",
      "Epoch 400 Loss: 1.4210854715202004e-12\n",
      "Epoch 500 Loss: 1.4210854715202004e-12\n",
      "Epoch 600 Loss: 1.4210854715202004e-12\n",
      "Epoch 700 Loss: 1.4210854715202004e-12\n",
      "Epoch 800 Loss: 1.4210854715202004e-12\n",
      "Epoch 900 Loss: 1.4210854715202004e-12\n",
      "Final Loss: 1.4210854715202004e-12\n",
      "Epoch 0 Loss: 1.3033463954925537\n",
      "Epoch 100 Loss: 2.2172486069393926e-09\n",
      "Epoch 200 Loss: 1.2825296380469808e-12\n",
      "Epoch 300 Loss: 1.2825296380469808e-12\n",
      "Epoch 400 Loss: 1.2825296380469808e-12\n",
      "Epoch 500 Loss: 1.2825296380469808e-12\n",
      "Epoch 600 Loss: 1.2825296380469808e-12\n",
      "Epoch 700 Loss: 1.2825296380469808e-12\n",
      "Epoch 800 Loss: 1.2825296380469808e-12\n",
      "Epoch 900 Loss: 1.2825296380469808e-12\n",
      "Final Loss: 1.2825296380469808e-12\n",
      "Epoch 0 Loss: 3.0321924686431885\n",
      "Epoch 100 Loss: 5.192934082742795e-09\n",
      "Epoch 200 Loss: 5.115907697472721e-13\n",
      "Epoch 300 Loss: 5.115907697472721e-13\n",
      "Epoch 400 Loss: 5.115907697472721e-13\n",
      "Epoch 500 Loss: 5.115907697472721e-13\n",
      "Epoch 600 Loss: 5.115907697472721e-13\n",
      "Epoch 700 Loss: 5.115907697472721e-13\n",
      "Epoch 800 Loss: 5.115907697472721e-13\n",
      "Epoch 900 Loss: 5.115907697472721e-13\n",
      "Final Loss: 5.115907697472721e-13\n",
      "Epoch 0 Loss: 3.387787342071533\n",
      "Epoch 100 Loss: 5.793513224716662e-09\n",
      "Epoch 200 Loss: 2.4016344468691386e-12\n",
      "Epoch 300 Loss: 2.4016344468691386e-12\n",
      "Epoch 400 Loss: 2.4016344468691386e-12\n",
      "Epoch 500 Loss: 2.4016344468691386e-12\n",
      "Epoch 600 Loss: 2.4016344468691386e-12\n",
      "Epoch 700 Loss: 2.4016344468691386e-12\n",
      "Epoch 800 Loss: 2.4016344468691386e-12\n",
      "Epoch 900 Loss: 2.4016344468691386e-12\n",
      "Final Loss: 2.4016344468691386e-12\n",
      "Epoch 0 Loss: 0.2597537934780121\n",
      "Epoch 100 Loss: 4.427000988016516e-10\n",
      "Epoch 200 Loss: 9.094947017729282e-13\n",
      "Epoch 300 Loss: 9.094947017729282e-13\n",
      "Epoch 400 Loss: 9.094947017729282e-13\n",
      "Epoch 500 Loss: 9.094947017729282e-13\n",
      "Epoch 600 Loss: 9.094947017729282e-13\n",
      "Epoch 700 Loss: 9.094947017729282e-13\n",
      "Epoch 800 Loss: 9.094947017729282e-13\n",
      "Epoch 900 Loss: 9.094947017729282e-13\n",
      "Final Loss: 9.094947017729282e-13\n",
      "Epoch 0 Loss: 2.7158331871032715\n",
      "Epoch 100 Loss: 4.625210436870475e-09\n",
      "Epoch 200 Loss: 2.0463630789890885e-12\n",
      "Epoch 300 Loss: 2.0463630789890885e-12\n",
      "Epoch 400 Loss: 2.0463630789890885e-12\n",
      "Epoch 500 Loss: 2.0463630789890885e-12\n",
      "Epoch 600 Loss: 2.0463630789890885e-12\n",
      "Epoch 700 Loss: 2.0463630789890885e-12\n",
      "Epoch 800 Loss: 2.0463630789890885e-12\n",
      "Epoch 900 Loss: 2.0463630789890885e-12\n",
      "Final Loss: 1.7195134205394424e-12\n",
      "Epoch 0 Loss: 0.8295252323150635\n",
      "Epoch 100 Loss: 1.423533291244894e-09\n",
      "Epoch 200 Loss: 1.4210854715202004e-14\n",
      "Epoch 300 Loss: 1.4210854715202004e-14\n",
      "Epoch 400 Loss: 1.4210854715202004e-14\n",
      "Epoch 500 Loss: 1.4210854715202004e-14\n",
      "Epoch 600 Loss: 1.4210854715202004e-14\n",
      "Epoch 700 Loss: 1.4210854715202004e-14\n",
      "Epoch 800 Loss: 1.4210854715202004e-14\n",
      "Epoch 900 Loss: 1.4210854715202004e-14\n",
      "Final Loss: 1.4210854715202004e-14\n",
      "Epoch 0 Loss: 2.603914260864258\n",
      "Epoch 100 Loss: 4.47245440682309e-09\n",
      "Epoch 200 Loss: 1.7195134205394424e-12\n",
      "Epoch 300 Loss: 1.7195134205394424e-12\n",
      "Epoch 400 Loss: 1.7195134205394424e-12\n",
      "Epoch 500 Loss: 1.7195134205394424e-12\n",
      "Epoch 600 Loss: 1.7195134205394424e-12\n",
      "Epoch 700 Loss: 1.7195134205394424e-12\n",
      "Epoch 800 Loss: 1.7195134205394424e-12\n",
      "Epoch 900 Loss: 1.7195134205394424e-12\n",
      "Final Loss: 1.7195134205394424e-12\n",
      "Epoch 0 Loss: 2.8613123893737793\n",
      "Epoch 100 Loss: 4.9133177526528016e-09\n",
      "Epoch 200 Loss: 2.7853275241795927e-12\n",
      "Epoch 300 Loss: 3.197442310920451e-12\n",
      "Epoch 400 Loss: 2.7853275241795927e-12\n",
      "Epoch 500 Loss: 2.7853275241795927e-12\n",
      "Epoch 600 Loss: 3.197442310920451e-12\n",
      "Epoch 700 Loss: 2.7853275241795927e-12\n",
      "Epoch 800 Loss: 2.7853275241795927e-12\n",
      "Epoch 900 Loss: 3.197442310920451e-12\n",
      "Final Loss: 2.7853275241795927e-12\n",
      "Epoch 0 Loss: 2.4076552391052246\n",
      "Epoch 100 Loss: 4.18234336052592e-09\n",
      "Epoch 200 Loss: 1.4210854715202004e-12\n",
      "Epoch 300 Loss: 1.4210854715202004e-12\n",
      "Epoch 400 Loss: 1.4210854715202004e-12\n",
      "Epoch 500 Loss: 1.4210854715202004e-12\n",
      "Epoch 600 Loss: 1.4210854715202004e-12\n",
      "Epoch 700 Loss: 1.4210854715202004e-12\n",
      "Epoch 800 Loss: 1.4210854715202004e-12\n",
      "Epoch 900 Loss: 1.4210854715202004e-12\n",
      "Final Loss: 1.4210854715202004e-12\n",
      "Epoch 0 Loss: 3.840017557144165\n",
      "Epoch 100 Loss: 6.600121338351528e-09\n",
      "Epoch 200 Loss: 5.403677505455562e-12\n",
      "Epoch 300 Loss: 5.403677505455562e-12\n",
      "Epoch 400 Loss: 5.403677505455562e-12\n",
      "Epoch 500 Loss: 5.403677505455562e-12\n",
      "Epoch 600 Loss: 5.403677505455562e-12\n",
      "Epoch 700 Loss: 5.403677505455562e-12\n",
      "Epoch 800 Loss: 5.403677505455562e-12\n",
      "Epoch 900 Loss: 5.403677505455562e-12\n",
      "Final Loss: 5.403677505455562e-12\n",
      "Epoch 0 Loss: 0.17139603197574615\n",
      "Epoch 100 Loss: 2.9467628337442875e-10\n",
      "Epoch 200 Loss: 2.4016344468691386e-12\n",
      "Epoch 300 Loss: 2.4016344468691386e-12\n",
      "Epoch 400 Loss: 2.4016344468691386e-12\n",
      "Epoch 500 Loss: 2.4016344468691386e-12\n",
      "Epoch 600 Loss: 2.4016344468691386e-12\n",
      "Epoch 700 Loss: 2.4016344468691386e-12\n",
      "Epoch 800 Loss: 2.4016344468691386e-12\n",
      "Epoch 900 Loss: 2.4016344468691386e-12\n",
      "Final Loss: 2.4016344468691386e-12\n",
      "Epoch 0 Loss: 0.3020634353160858\n",
      "Epoch 100 Loss: 5.049436424542364e-10\n",
      "Epoch 200 Loss: 5.115907697472721e-13\n",
      "Epoch 300 Loss: 5.115907697472721e-13\n",
      "Epoch 400 Loss: 5.115907697472721e-13\n",
      "Epoch 500 Loss: 5.115907697472721e-13\n",
      "Epoch 600 Loss: 5.115907697472721e-13\n",
      "Epoch 700 Loss: 5.115907697472721e-13\n",
      "Epoch 800 Loss: 5.115907697472721e-13\n",
      "Epoch 900 Loss: 5.115907697472721e-13\n",
      "Final Loss: 5.115907697472721e-13\n",
      "Epoch 0 Loss: 1.2840272188186646\n",
      "Epoch 100 Loss: 2.2004407185249875e-09\n",
      "Epoch 200 Loss: 3.552713678800501e-13\n",
      "Epoch 300 Loss: 3.552713678800501e-13\n",
      "Epoch 400 Loss: 3.552713678800501e-13\n",
      "Epoch 500 Loss: 3.552713678800501e-13\n",
      "Epoch 600 Loss: 3.552713678800501e-13\n",
      "Epoch 700 Loss: 3.552713678800501e-13\n",
      "Epoch 800 Loss: 3.552713678800501e-13\n",
      "Epoch 900 Loss: 3.552713678800501e-13\n",
      "Final Loss: 3.552713678800501e-13\n",
      "Epoch 0 Loss: 2.6404049396514893\n",
      "Epoch 100 Loss: 4.504400408222864e-09\n",
      "Epoch 200 Loss: 2.9878322038712213e-12\n",
      "Epoch 300 Loss: 2.9878322038712213e-12\n",
      "Epoch 400 Loss: 2.9878322038712213e-12\n",
      "Epoch 500 Loss: 2.9878322038712213e-12\n",
      "Epoch 600 Loss: 2.9878322038712213e-12\n",
      "Epoch 700 Loss: 2.9878322038712213e-12\n",
      "Epoch 800 Loss: 2.9878322038712213e-12\n",
      "Epoch 900 Loss: 2.9878322038712213e-12\n",
      "Final Loss: 2.9878322038712213e-12\n",
      "Epoch 0 Loss: 0.2716327905654907\n",
      "Epoch 100 Loss: 4.5787729163748736e-10\n",
      "Epoch 200 Loss: 2.589928271845565e-12\n",
      "Epoch 300 Loss: 2.589928271845565e-12\n",
      "Epoch 400 Loss: 2.589928271845565e-12\n",
      "Epoch 500 Loss: 2.589928271845565e-12\n",
      "Epoch 600 Loss: 2.589928271845565e-12\n",
      "Epoch 700 Loss: 2.589928271845565e-12\n",
      "Epoch 800 Loss: 2.589928271845565e-12\n",
      "Epoch 900 Loss: 2.589928271845565e-12\n",
      "Final Loss: 2.589928271845565e-12\n",
      "Epoch 0 Loss: 0.6558975577354431\n",
      "Epoch 100 Loss: 1.1301040103717241e-09\n",
      "Epoch 200 Loss: 2.4016344468691386e-12\n",
      "Epoch 300 Loss: 2.4016344468691386e-12\n",
      "Epoch 400 Loss: 2.4016344468691386e-12\n",
      "Epoch 500 Loss: 2.4016344468691386e-12\n",
      "Epoch 600 Loss: 2.4016344468691386e-12\n",
      "Epoch 700 Loss: 2.4016344468691386e-12\n",
      "Epoch 800 Loss: 2.4016344468691386e-12\n",
      "Epoch 900 Loss: 2.4016344468691386e-12\n",
      "Final Loss: 2.4016344468691386e-12\n",
      "Epoch 0 Loss: 0.36532434821128845\n",
      "Epoch 100 Loss: 6.148184183984995e-10\n",
      "Epoch 200 Loss: 1.4210854715202004e-12\n",
      "Epoch 300 Loss: 1.4210854715202004e-12\n",
      "Epoch 400 Loss: 1.4210854715202004e-12\n",
      "Epoch 500 Loss: 1.4210854715202004e-12\n",
      "Epoch 600 Loss: 1.4210854715202004e-12\n",
      "Epoch 700 Loss: 1.4210854715202004e-12\n",
      "Epoch 800 Loss: 1.4210854715202004e-12\n",
      "Epoch 900 Loss: 1.4210854715202004e-12\n",
      "Final Loss: 1.4210854715202004e-12\n",
      "Epoch 0 Loss: 5.649966239929199\n",
      "Epoch 100 Loss: 9.695725111669162e-09\n",
      "Epoch 200 Loss: 3.4141578453272814e-12\n",
      "Epoch 300 Loss: 3.4141578453272814e-12\n",
      "Epoch 400 Loss: 3.4141578453272814e-12\n",
      "Epoch 500 Loss: 3.4141578453272814e-12\n",
      "Epoch 600 Loss: 3.4141578453272814e-12\n",
      "Epoch 700 Loss: 3.4141578453272814e-12\n",
      "Epoch 800 Loss: 3.4141578453272814e-12\n",
      "Epoch 900 Loss: 3.4141578453272814e-12\n",
      "Final Loss: 3.4141578453272814e-12\n",
      "Epoch 0 Loss: 3.6604058742523193\n",
      "Epoch 100 Loss: 6.2749485607582756e-09\n",
      "Epoch 200 Loss: 5.130118552187923e-12\n",
      "Epoch 300 Loss: 5.130118552187923e-12\n",
      "Epoch 400 Loss: 5.130118552187923e-12\n",
      "Epoch 500 Loss: 5.130118552187923e-12\n",
      "Epoch 600 Loss: 5.130118552187923e-12\n",
      "Epoch 700 Loss: 5.130118552187923e-12\n",
      "Epoch 800 Loss: 5.130118552187923e-12\n",
      "Epoch 900 Loss: 5.130118552187923e-12\n",
      "Final Loss: 5.130118552187923e-12\n",
      "Epoch 0 Loss: 0.0027964822947978973\n",
      "Epoch 100 Loss: 3.4141578453272814e-12\n",
      "Epoch 200 Loss: 1.7195134205394424e-12\n",
      "Epoch 300 Loss: 1.879385536085465e-12\n",
      "Epoch 400 Loss: 1.879385536085465e-12\n",
      "Epoch 500 Loss: 1.879385536085465e-12\n",
      "Epoch 600 Loss: 1.879385536085465e-12\n",
      "Epoch 700 Loss: 1.879385536085465e-12\n",
      "Epoch 800 Loss: 1.879385536085465e-12\n",
      "Epoch 900 Loss: 1.879385536085465e-12\n",
      "Final Loss: 1.879385536085465e-12\n",
      "Epoch 0 Loss: 0.04553724825382233\n",
      "Epoch 100 Loss: 7.887379638304992e-11\n",
      "Epoch 200 Loss: 4.352074256530614e-12\n",
      "Epoch 300 Loss: 4.352074256530614e-12\n",
      "Epoch 400 Loss: 4.352074256530614e-12\n",
      "Epoch 500 Loss: 4.352074256530614e-12\n",
      "Epoch 600 Loss: 4.352074256530614e-12\n",
      "Epoch 700 Loss: 4.352074256530614e-12\n",
      "Epoch 800 Loss: 4.352074256530614e-12\n",
      "Epoch 900 Loss: 4.352074256530614e-12\n",
      "Final Loss: 4.352074256530614e-12\n",
      "Epoch 0 Loss: 2.424467086791992\n",
      "Epoch 100 Loss: 4.136214926120374e-09\n",
      "Epoch 200 Loss: 1.566746732351021e-12\n",
      "Epoch 300 Loss: 1.566746732351021e-12\n",
      "Epoch 400 Loss: 1.566746732351021e-12\n",
      "Epoch 500 Loss: 1.566746732351021e-12\n",
      "Epoch 600 Loss: 1.566746732351021e-12\n",
      "Epoch 700 Loss: 1.566746732351021e-12\n",
      "Epoch 800 Loss: 1.566746732351021e-12\n",
      "Epoch 900 Loss: 1.566746732351021e-12\n",
      "Final Loss: 1.566746732351021e-12\n",
      "Epoch 0 Loss: 1.3919686079025269\n",
      "Epoch 100 Loss: 2.3713973007488676e-09\n",
      "Epoch 200 Loss: 2.9878322038712213e-12\n",
      "Epoch 300 Loss: 2.9878322038712213e-12\n",
      "Epoch 400 Loss: 2.9878322038712213e-12\n",
      "Epoch 500 Loss: 2.9878322038712213e-12\n",
      "Epoch 600 Loss: 2.9878322038712213e-12\n",
      "Epoch 700 Loss: 2.9878322038712213e-12\n",
      "Epoch 800 Loss: 2.9878322038712213e-12\n",
      "Epoch 900 Loss: 2.9878322038712213e-12\n",
      "Final Loss: 2.9878322038712213e-12\n",
      "Epoch 0 Loss: 0.6660003662109375\n",
      "Epoch 100 Loss: 1.13411502411509e-09\n",
      "Epoch 200 Loss: 3.637978807091713e-12\n",
      "Epoch 300 Loss: 3.637978807091713e-12\n",
      "Epoch 400 Loss: 3.637978807091713e-12\n",
      "Epoch 500 Loss: 3.637978807091713e-12\n",
      "Epoch 600 Loss: 3.637978807091713e-12\n",
      "Epoch 700 Loss: 3.637978807091713e-12\n",
      "Epoch 800 Loss: 3.637978807091713e-12\n",
      "Epoch 900 Loss: 3.637978807091713e-12\n",
      "Final Loss: 3.637978807091713e-12\n",
      "Epoch 0 Loss: 1.4259765148162842\n",
      "Epoch 100 Loss: 2.494871864655579e-09\n",
      "Epoch 200 Loss: 3.637978807091713e-12\n",
      "Epoch 300 Loss: 3.637978807091713e-12\n",
      "Epoch 400 Loss: 3.637978807091713e-12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, i[j])\n\u001b[1;32m     17\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 18\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/mixbits/.venv/lib64/python3.9/site-packages/torch/optim/optimizer.py:471\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;66;03m# call optimizer step pre hooks\u001b[39;00m\n\u001b[0;32m--> 471\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pre_hook \u001b[38;5;129;01min\u001b[39;00m \u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_global_optimizer_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    475\u001b[0m         result \u001b[38;5;241m=\u001b[39m pre_hook(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[1;32m    476\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "labels_models_map = {}\n",
    "\n",
    "labels_to_train = random.sample(all_possible_labels, 50)\n",
    "\n",
    "\n",
    "for i in labels_to_train:\n",
    "    model = Lut4()\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.003)\n",
    "    for epoch in range(1000):\n",
    "        for j in range(len(inputs)):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs[j])\n",
    "            loss = criterion(outputs, i[j])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch} Loss: {loss.item()}')\n",
    "\n",
    "    print(f'Final Loss: {loss.item()}')\n",
    "    labels_models_map[i] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(model):\n",
    "    predictions = []\n",
    "    for i in range(16):\n",
    "        x = inputs[i]\n",
    "        output = model(x)\n",
    "        predictions.append(output.item())   \n",
    "    return predictions\n",
    "\n",
    "# for labels, model in labels_models_map.items():\n",
    "#     predictions = predict(model)\n",
    "#     if predictions == list(labels):\n",
    "#         print(f'found model for labels {labels}')\n",
    "#         break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found model for labels tensor([-1.,  1., -1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1.])\n",
      "found model for labels tensor([-1., -1.,  1.,  1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1.,\n",
      "        -1.,  1.])\n",
      "found model for labels tensor([ 1.,  1.,  1.,  1., -1., -1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,\n",
      "         1., -1.])\n",
      "found model for labels tensor([-1., -1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,  1., -1., -1., -1.,\n",
      "        -1.,  1.])\n",
      "found model for labels tensor([-1.,  1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1.,  1., -1., -1.,\n",
      "        -1., -1.])\n",
      "found model for labels tensor([ 1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,\n",
      "        -1., -1.])\n",
      "found model for labels tensor([ 1., -1.,  1.,  1.,  1., -1., -1.,  1., -1., -1., -1.,  1., -1.,  1.,\n",
      "        -1., -1.])\n",
      "found model for labels tensor([-1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1.,\n",
      "        -1.,  1.])\n",
      "found model for labels tensor([ 1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1., -1.,\n",
      "         1.,  1.])\n",
      "found model for labels tensor([-1.,  1., -1., -1., -1.,  1., -1., -1., -1., -1., -1.,  1., -1., -1.,\n",
      "         1., -1.])\n",
      "found model for labels tensor([ 1.,  1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,\n",
      "         1., -1.])\n",
      "found model for labels tensor([ 1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,\n",
      "         1.,  1.])\n",
      "found model for labels tensor([ 1.,  1., -1., -1.,  1., -1., -1., -1., -1., -1.,  1., -1.,  1.,  1.,\n",
      "         1., -1.])\n",
      "found model for labels tensor([ 1., -1.,  1., -1.,  1.,  1., -1., -1.,  1., -1., -1., -1.,  1., -1.,\n",
      "         1.,  1.])\n",
      "found model for labels tensor([-1., -1., -1., -1.,  1., -1.,  1., -1., -1., -1.,  1., -1., -1., -1.,\n",
      "         1.,  1.])\n",
      "found model for labels tensor([-1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1., -1., -1.,\n",
      "        -1.,  1.])\n",
      "found model for labels tensor([-1., -1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
      "        -1., -1.])\n",
      "found model for labels tensor([-1., -1.,  1., -1.,  1., -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.,\n",
      "        -1.,  1.])\n",
      "found model for labels tensor([ 1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,\n",
      "        -1., -1.])\n",
      "found model for labels tensor([-1., -1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,\n",
      "        -1.,  1.])\n",
      "found model for labels tensor([-1.,  1., -1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1.,\n",
      "         1., -1.])\n",
      "found model for labels tensor([ 1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
      "         1., -1.])\n",
      "found model for labels tensor([-1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1.,\n",
      "        -1., -1.])\n",
      "found model for labels tensor([-1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,\n",
      "        -1., -1.])\n",
      "found model for labels tensor([ 1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        -1.,  1.])\n",
      "found model for labels tensor([-1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,\n",
      "         1., -1.])\n",
      "found model for labels tensor([ 1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,\n",
      "        -1., -1.])\n",
      "found model for labels tensor([ 1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1.,\n",
      "         1.,  1.])\n",
      "found model for labels tensor([ 1., -1.,  1.,  1.,  1., -1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,\n",
      "         1., -1.])\n",
      "found model for labels tensor([ 1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,\n",
      "        -1.,  1.])\n",
      "found model for labels tensor([-1., -1., -1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,  1.,  1.,\n",
      "        -1.,  1.])\n",
      "found model for labels tensor([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1.,  1.,  1.,  1.,\n",
      "         1., -1.])\n",
      "found model for labels tensor([ 1.,  1., -1.,  1.,  1., -1., -1.,  1., -1., -1., -1., -1., -1.,  1.,\n",
      "         1., -1.])\n",
      "found model for labels tensor([-1., -1., -1.,  1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1.,\n",
      "        -1., -1.])\n",
      "found model for labels tensor([-1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,\n",
      "         1., -1.])\n",
      "found model for labels tensor([ 1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,  1., -1.,\n",
      "        -1., -1.])\n"
     ]
    }
   ],
   "source": [
    "def visualise_binary_result(x):\n",
    "    out = \"\"\n",
    "    for i in x:\n",
    "        if i == 1:\n",
    "            out += \"1\"\n",
    "        else:\n",
    "            out += \"0\"\n",
    "    return out\n",
    "\n",
    "def count_errors(x, y):\n",
    "    errors = 0\n",
    "    for i in range(len(x)):\n",
    "        if x[i] != y[i]:\n",
    "            errors += 1\n",
    "    return errors\n",
    "\n",
    "for labels, model in labels_models_map.items():\n",
    "    predictions = predict(model)\n",
    "    binary_labels = [l == 1 for l in labels]\n",
    "    binary_predictions = [p > 0 for p in predictions]\n",
    "    if binary_labels == binary_predictions:\n",
    "        print(f'found model for labels {labels}')\n",
    "    else:\n",
    "        print(\"-\"*30)\n",
    "        print(visualise_binary_result(binary_labels))\n",
    "        print(visualise_binary_result(binary_predictions))\n",
    "        print(count_errors(binary_labels, binary_predictions))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1]]) tensor([[-1]])\n",
      "tensor([[-1]]) tensor([[-1]])\n",
      "tensor([[-1]]) tensor([[-1]])\n",
      "tensor([[-1]]) tensor([[1]])\n",
      "tensor([[-1]]) tensor([[-1]])\n",
      "tensor([[-1]]) tensor([[-1]])\n",
      "tensor([[-1]]) tensor([[-1]])\n",
      "tensor([[-1]]) tensor([[1]])\n",
      "tensor([[1]]) tensor([[-1]])\n",
      "tensor([[1]]) tensor([[-1]])\n",
      "tensor([[1]]) tensor([[-1]])\n",
      "tensor([[1]]) tensor([[1]])\n",
      "tensor([[1]]) tensor([[-1]])\n",
      "tensor([[1]]) tensor([[-1]])\n",
      "tensor([[1]]) tensor([[-1]])\n",
      "tensor([[1]]) tensor([[1]])\n"
     ]
    }
   ],
   "source": [
    "for labels, model in labels_models_map.items():\n",
    "    model.debug = True\n",
    "    predictions = predict(model)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3000]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model((-1, 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
